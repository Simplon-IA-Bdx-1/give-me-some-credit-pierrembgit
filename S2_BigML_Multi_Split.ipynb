{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe la librairie bigml\n",
    "from bigml.api import BigML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se connecte à bigml\n",
    "api = BigML(project='project/5d94a32e42129f2e16000232')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une source à partir du csv\n",
    "source_train = api.create_source('storage/source_dataset_train_full.csv')\n",
    "source_test = api.create_source('storage/source_dataset_test.csv')\n",
    "api.ok(source_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On crée un dataset à partir de la source puis on split (DATASET de TRAIN)\n",
    "dataset_train_full = api.create_dataset(source_train, {\"name\": \"Dataset Train Full\"})\n",
    "dataset_train_train = api.create_dataset(dataset_train_full, {\"name\": \"Dataset Train Train\", \"sample_rate\": 0.8, \"seed\": \"my seed\"})\n",
    "dataset_train_test = api.create_dataset(dataset_train_full, {\"name\": \"Dataset Train Test\", \"sample_rate\": 0.8 , \"seed\": \"my seed\", \"out_of_bag\": True})\n",
    "\n",
    "# On crée un dataset à partir de la source (DATASET de TEST)\n",
    "dataset_test = api.create_dataset(source_test, {\"name\": \"Dataset Test\"})\n",
    "api.ok(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 AUC = 0.5\n",
      " 20 AUC = 0.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "{'bytes': 0, 'code': -1, 'elapsed': 515, 'error': -3050, 'field_errors': {}, 'message': 'Sampling, filter, query or generator yielded no rows', 'progress': 0, 'row_format_errors': {'total': 0}, 'serialized_rows': 0}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-676ce2ff9f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_train_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset Train Train \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"my seed\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"objective_field\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"SeriousDlqin2yrs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Ensemble \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bigml/resourcehandler.py\u001b[0m in \u001b[0;36mok\u001b[0;34m(self, resource, query_string, wait_time, retries, raise_on_error)\u001b[0m\n\u001b[1;32m    507\u001b[0m                                            \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                                            \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                                            api=self))\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bigml/resourcehandler.py\u001b[0m in \u001b[0;36mcheck_resource\u001b[0;34m(resource, get_method, query_string, wait_time, retries, raise_on_error, api)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAULTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_exponential_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# retries for the finished status use a query string that gets the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: {'bytes': 0, 'code': -1, 'elapsed': 515, 'error': -3050, 'field_errors': {}, 'message': 'Sampling, filter, query or generator yielded no rows', 'progress': 0, 'row_format_errors': {'total': 0}, 'serialized_rows': 0}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "for i in range(1,10):\n",
    "    dataset_train_train = api.create_dataset(dataset_train_train, {\"name\": \"Dataset Train Train \" + str(i/10), \"sample_rate\": i/10, \"seed\": \"my seed\"})\n",
    "    api.ok(dataset_train_train)\n",
    "    \n",
    "    ensemble = api.create_ensemble(dataset_train_train, {\"objective_field\" : \"SeriousDlqin2yrs\", \"name\": \"Ensemble \" + str(i/10)})\n",
    "    api.ok(ensemble)\n",
    "    \n",
    "#     batch_prediction_all_fields = api.create_batch_prediction(ensemble, dataset_train_test, {\"name\": \"Batch Prediction All Fields \" + str(i/10), \"prediction_name\" : \"SeriousDlqin2yrs_Predic\", \"all_fields\": True, \"probabilities\": True,})\n",
    "#     api.ok(batch_prediction_all_fields)\n",
    "    \n",
    "    evaluation = api.create_evaluation(ensemble, dataset_train_test)\n",
    "    api.ok(evaluation)\n",
    "    AUC = evaluation['object']['result']['model']['average_area_under_roc_curve']\n",
    "    print(f\" {i*10} AUC = {AUC}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lance un ensemble\n",
    "ensemble = api.create_ensemble(dataset_train_train, {\"objective_field\" : \"SeriousDlqin2yrs\", \"name\": \"Ensemble\"})\n",
    "ensemble_full = api.create_ensemble(dataset_train_full, {\"objective_field\" : \"SeriousDlqin2yrs\", \"name\": \"Ensemble Full\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lance un batch sur le dataset TEST\n",
    "batch_prediction = api.create_batch_prediction(ensemble_full, dataset_test, {\"name\": \"Batch Prediction\", \"output_fields\": [\"Id\"], \"probabilities\": True})\n",
    "# On vérifie le bon fonctionnement du batch\n",
    "api.ok(batch_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lance un batch sur le dataset Train de validation\n",
    "batch_prediction_all_fields = api.create_batch_prediction(ensemble, dataset_train_test, {\"name\": \"Batch Prediction All Fields\", \"prediction_name\" : \"SeriousDlqin2yrs_Predic\", \"all_fields\": True, \"probabilities\": True,})\n",
    "# On vérifie le bon fonctionnement du batch\n",
    "api.ok(batch_prediction_all_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère le ROC AUC via une évaluation\n",
    "evaluation = api.create_evaluation(ensemble, dataset_train_test)\n",
    "api.ok(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère le ROC AUC via une évaluation\n",
    "evaluation_full = api.create_evaluation(ensemble_full, dataset_train_test)\n",
    "api.ok(evaluation_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche l'AUC depuis l'évaluation\n",
    "#api.pprint(evaluation['object']['result'])\n",
    "AUC = evaluation['object']['result']['model']['average_area_under_roc_curve']\n",
    "print(f\" AUC = {AUC}\")\n",
    "\n",
    "AUC = evaluation_full['object']['result']['model']['average_area_under_roc_curve']\n",
    "print(f\" AUC = {AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge les csv du batch et des datasets ainsi que le json de l'évaluation\n",
    "api.download_batch_prediction(batch_prediction, filename='storage/batch_prediction.csv')\n",
    "api.download_batch_prediction(batch_prediction_all_fields, filename='storage/batch_prediction_all_fields.csv')\n",
    "api.download_dataset(dataset_train_full, filename='storage/dataset_train_full.csv')\n",
    "api.download_dataset(dataset_train_train, filename='storage/dataset_train_train.csv')\n",
    "api.download_dataset(dataset_train_test, filename='storage/dataset_train_test.csv')\n",
    "api.download_dataset(dataset_test, filename='storage/dataset_test.csv')\n",
    "api.export(evaluation, filename='storage/my_evaluation.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
