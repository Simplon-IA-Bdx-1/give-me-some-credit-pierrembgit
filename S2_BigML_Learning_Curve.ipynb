{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve\n",
    "\n",
    "## Objectifs:\n",
    " - Calculer l'AUC avec un Ensemble sur le VAL SET\n",
    " - Calculer l'AUC avec un Deepnet sur le VAL SET\n",
    " - Calculer l'AUC avec un Ensemble sur le TRAIN SET\n",
    " - Calculer l'AUC avec un Deepnet sur le TRAIN SET\n",
    " - Représenter les données dans trois graphiques :\n",
    "  - Learning Curve (AUC / % Split) Ensemble vs Deepnet\n",
    "  - Learning Curve (AUC / % Split) Ensemble : Training Error / Dev Error\n",
    "  - Learning Curve (AUC / % Split) Deepnet : Training Error / Dev Error\n",
    "\n",
    "Les id des resources sont stockés dans une jar pickle ce qui permet de réduire le temps d'exécution du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# On importe la librairie bigml\n",
    "from bigml.api import BigML\n",
    "from os import path\n",
    "import time\n",
    "import pprint\n",
    "from pickle import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# Configuration de pickle\n",
    "jar_filename = 'storage/gmsc-picklejar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Connexion à BigML\n",
    "api = BigML(project='project/5d94a32e42129f2e16000232')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction permettant d'ouvrir une jar avec pickle\n",
    "def read_jar(jar_filename):\n",
    "    if path.exists(jar_filename): # Si le fichier existe on l'ouvre\n",
    "        with open(jar_filename, 'rb') as file:\n",
    "            project_data = load(file)\n",
    "    else: # Si le fichier n'existe pas un crée un dictionnaire vide\n",
    "        project_data = {}\n",
    "    return project_data\n",
    "\n",
    "# Fonction permettant de sauvegarder des données dans une jar avec pickle\n",
    "def save_jar(jar_filename, project_data):\n",
    "    with open(jar_filename, 'wb') as file:\n",
    "        dump(project_data,file)\n",
    "    return project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Source TRAIN FULL : OK\n",
      " >> Source TEST : OK\n",
      " >>> Dataset TRAIN FULL : OK\n",
      " >>> Dataset TRAIN TRAIN : OK\n",
      " >>> Dataset TRAIN TEST : OK\n",
      " >>> Dataset TEST : OK\n",
      " >>>> Evaluation 1 : OK\n",
      " >>>> Evaluation 2 : OK\n",
      " >>>> Evaluation 3 : OK\n",
      " >>>> Evaluation 4 : OK\n",
      " >>>> Evaluation 5 : OK\n",
      " >>>> Evaluation 6 : OK\n",
      " >>>> Evaluation 7 : OK\n",
      " >>>> Evaluation 8 : OK\n",
      " >>>> Evaluation 9 : OK\n",
      " >>>> Evaluation 10 : OK\n",
      " >>>>> Terminé\n"
     ]
    }
   ],
   "source": [
    "# Learning Curves\n",
    "project_data = read_jar(jar_filename)\n",
    "\n",
    " # On boucle sur l'axe des abscisses / Taille du dataset de train / % de ligne du dataset de train_train\n",
    "for i in range(0,11):\n",
    "    # Création du dictionnaire\n",
    "    if i not in project_data:\n",
    "        project_data[i] = {}\n",
    "    ## Dataset\n",
    "    if 'dataset' not in project_data[i]:\n",
    "        project_data[i]['dataset'] = {}\n",
    "    ## Modèles\n",
    "    if 'models' not in project_data[i]:\n",
    "        project_data[i]['models'] = {}\n",
    "    if 'Ensemble' not in project_data[i]['models']:\n",
    "        project_data[i]['models']['Ensemble'] = {}\n",
    "    if 'Deepnet' not in project_data[i]['models']:\n",
    "        project_data[i]['models']['Deepnet'] = {}\n",
    "    ## Evaluations\n",
    "    if 'evaluations' not in project_data[i]:\n",
    "        project_data[i]['evaluations'] = {}\n",
    "    ### Val set\n",
    "    if 'val_set' not in project_data[i]['evaluations']:\n",
    "        project_data[i]['evaluations']['val_set'] = {}\n",
    "    if 'Ensemble' not in project_data[i]['evaluations']['val_set']:\n",
    "        project_data[i]['evaluations']['val_set']['Ensemble'] = {}\n",
    "    if 'Deepnet' not in project_data[i]['evaluations']['val_set']:\n",
    "        project_data[i]['evaluations']['val_set']['Deepnet'] = {}\n",
    "    ### Train set  \n",
    "    if 'train_set' not in project_data[i]['evaluations']:\n",
    "        project_data[i]['evaluations']['train_set'] = {}\n",
    "    if 'Ensemble' not in project_data[i]['evaluations']['train_set']:\n",
    "        project_data[i]['evaluations']['train_set']['Ensemble'] = {}\n",
    "    if 'Deepnet' not in project_data[i]['evaluations']['train_set']:\n",
    "        project_data[i]['evaluations']['train_set']['Deepnet'] = {}\n",
    "        \n",
    "    if i == 0 :\n",
    "        # On crée une source à partir du fichier csv DATASET TRAIN FULL\n",
    "        if 'source_train' not in project_data[0]:\n",
    "            source_train = api.create_source('storage/source_dataset_train_full.csv')\n",
    "            project_data[0]['source_train'] = source_train['resource']\n",
    "        else:\n",
    "            source_train = api.get_source(project_data[0]['source_train'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(source_train)\n",
    "        print(f\" >> Source TRAIN FULL : OK\")\n",
    "        \n",
    "        # On crée une source à partir du fichier csv DATASET TEST\n",
    "        if 'source_test' not in project_data[0]:\n",
    "            source_test = api.create_source('storage/source_dataset_test.csv')\n",
    "            project_data[0]['source_test'] = source_test['resource']\n",
    "        else:\n",
    "            source_test = api.get_source(project_data[0]['source_test'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(source_test)\n",
    "        print(f\" >> Source TEST : OK\")\n",
    "        \n",
    "        # On crére le dataset TRAIN FULL à partir de la source\n",
    "        if 'dataset_train_full' not in project_data[0]:\n",
    "            dataset_train_full = api.create_dataset(source_train, {\"name\": \"Dataset Train Full\"})\n",
    "            project_data[0]['dataset_train_full'] = dataset_train_full['resource']\n",
    "        else:\n",
    "            dataset_train_full = api.get_dataset(project_data[0]['dataset_train_full'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(dataset_train_full)\n",
    "        print(f\" >>> Dataset TRAIN FULL : OK\")\n",
    "        \n",
    "        # On crére le dataset TRAIN TRAIN à partir de la source\n",
    "        if 'dataset_train_train' not in project_data[0]:\n",
    "            dataset_train_train = api.create_dataset(dataset_train_full, {\"name\": \"Dataset Train Train\", \"sample_rate\": 0.8, \"seed\": \"my seed\"})\n",
    "            project_data[0]['dataset_train_train'] = dataset_train_train['resource']\n",
    "        else:\n",
    "            dataset_train_train = api.get_dataset(project_data[0]['dataset_train_train'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(dataset_train_train)\n",
    "        print(f\" >>> Dataset TRAIN TRAIN : OK\")\n",
    "        \n",
    "        # On crére le dataset TRAIN TEST à partir de la source\n",
    "        if 'dataset_train_test' not in project_data[0]:\n",
    "            dataset_train_test = api.create_dataset(dataset_train_full, {\"name\": \"Dataset Train Test\", \"sample_rate\": 0.8 , \"seed\": \"my seed\", \"out_of_bag\": True})\n",
    "            project_data[0]['dataset_train_test'] = dataset_train_train['resource']\n",
    "        else:\n",
    "            dataset_train_test = api.get_dataset(project_data[0]['dataset_train_test'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(dataset_train_test)\n",
    "        print(f\" >>> Dataset TRAIN TEST : OK\")\n",
    "        \n",
    "        # On crée le dataset TEST à partir de la source\n",
    "        if 'dataset_test' not in project_data[0]:\n",
    "            dataset_test = api.create_dataset(source_test, {\"name\": \"Dataset Test\"})\n",
    "            project_data[0]['dataset_test'] = dataset_test['resource']\n",
    "        else:\n",
    "            dataset_test = api.get_dataset(project_data[0]['dataset_test'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(dataset_test)\n",
    "        print(f\" >>> Dataset TEST : OK\")\n",
    "   \n",
    "    else:\n",
    "        # Dataset \n",
    "        ## Création ou charement des dataset\n",
    "        if 'dataset_id' not in project_data[i]['dataset']: # Le dataset n'existe pas, on le crée\n",
    "            # On crée le nouveau dataset de train à partir du dataset de train initial\n",
    "            dataset_train_train_split = api.create_dataset(dataset_train_train, {\"name\": \"Dataset Train Train \" + str(i/10), \"sample_rate\": i/10, \"seed\": \"my seed\"})\n",
    "            project_data[i]['dataset']['dataset_id'] = dataset_train_train_split['resource']\n",
    "        else: # Le dataset existe, on le charge\n",
    "            # On récupère le dataset de train\n",
    "            dataset_train_train_split = api.get_dataset(project_data[i]['dataset']['dataset_id'])\n",
    "        save_jar(jar_filename, project_data)\n",
    "        api.ok(dataset_train_train_split)\n",
    "        \n",
    "        # Modèles\n",
    "        ## Création ou chargement des Ensemble\n",
    "        if 'model_id' not in project_data[i]['models']['Ensemble']:\n",
    "            ensemble = api.create_ensemble(dataset_train_train_split, {\"objective_field\" : \"SeriousDlqin2yrs\", \"name\": \"Ensemble \" + str(i/10)})\n",
    "            project_data[i]['models']['Ensemble']['model_id'] = ensemble['resource']\n",
    "            save_jar(jar_filename, project_data)\n",
    "        else:\n",
    "            ensemble = api.get_ensemble(project_data[i]['models']['Ensemble']['model_id'])\n",
    "        api.ok(ensemble)\n",
    "        \n",
    "        ## Création ou chargement des Deepnet\n",
    "        if 'model_id' not in project_data[i]['models']['Deepnet']:\n",
    "            deepnet = api.create_deepnet(dataset_train_train_split, {\"objective_field\" : \"SeriousDlqin2yrs\", \"name\": \"Deepnet \" + str(i/10)})\n",
    "            project_data[i]['models']['Deepnet']['model_id'] = deepnet['resource']\n",
    "            save_jar(jar_filename, project_data)\n",
    "        else:\n",
    "            deepnet = api.get_deepnet(project_data[i]['models']['Deepnet']['model_id'])\n",
    "        api.ok(deepnet)\n",
    "\n",
    "        # Evaluations\n",
    "        ## VAL SET\n",
    "        ### Ensemble\n",
    "        if 'evaluation_id' not in project_data[i]['evaluations']['val_set']['Ensemble']:\n",
    "            evaluation_valset_ensemble = api.create_evaluation(ensemble, dataset_train_test)\n",
    "            project_data[i]['evaluations']['val_set']['Ensemble']['evaluation_id'] = evaluation_valset_ensemble['resource']\n",
    "            save_jar(jar_filename, project_data)\n",
    "            api.ok(evaluation_valset_ensemble)\n",
    "            project_data[i]['evaluations']['val_set']['Ensemble']['auc'] = evaluation_valset_ensemble['object']['result']['model']['average_area_under_roc_curve']\n",
    "        else:\n",
    "            evaluation_valset_ensemble = api.get_evaluation(project_data[i]['evaluations']['val_set']['Ensemble']['evaluation_id'])\n",
    "            api.ok(evaluation_valset_ensemble)\n",
    "            project_data[i]['evaluations']['val_set']['Ensemble']['auc'] = evaluation_valset_ensemble['object']['result']['model']['average_area_under_roc_curve']\n",
    "            save_jar(jar_filename, project_data)\n",
    "        \n",
    "        ### Deepnet\n",
    "        if 'evaluation_id' not in project_data[i]['evaluations']['val_set']['Deepnet']:\n",
    "            evaluation_valset_deepnet = api.create_evaluation(deepnet, dataset_train_test)\n",
    "            project_data[i]['evaluations']['val_set']['Deepnet']['evaluation_id'] = evaluation_valset_deepnet['resource']\n",
    "            save_jar(jar_filename, project_data)\n",
    "            api.ok(evaluation_valset_deepnet)\n",
    "            project_data[i]['evaluations']['val_set']['Deepnet']['auc'] = evaluation_valset_deepnet['object']['result']['model']['average_area_under_roc_curve']\n",
    "        else:\n",
    "            evaluation_valset_deepnet = api.get_evaluation(project_data[i]['evaluations']['val_set']['Deepnet']['evaluation_id'])\n",
    "            api.ok(evaluation_valset_deepnet)\n",
    "            project_data[i]['evaluations']['val_set']['Deepnet']['auc'] = evaluation_valset_deepnet['object']['result']['model']['average_area_under_roc_curve']\n",
    "            save_jar(jar_filename, project_data)\n",
    "\n",
    "        ## TRAIN SET\n",
    "        ### Ensemble\n",
    "        if 'evaluation_id' not in project_data[i]['evaluations']['train_set']['Ensemble']:\n",
    "            evaluation_trainset_ensemble = api.create_evaluation(ensemble, dataset_train_train_split)\n",
    "            project_data[i]['evaluations']['train_set']['Ensemble']['evaluation_id'] = evaluation_trainset_ensemble['resource']\n",
    "            save_jar(jar_filename, project_data)\n",
    "            api.ok(evaluation_trainset_ensemble)\n",
    "            project_data[i]['evaluations']['train_set']['Ensemble']['auc'] = evaluation_trainset_ensemble['object']['result']['model']['average_area_under_roc_curve']\n",
    "        else:\n",
    "            evaluation_trainset_ensemble = api.get_evaluation(project_data[i]['evaluations']['train_set']['Ensemble']['evaluation_id'])\n",
    "            api.ok(evaluation_trainset_ensemble)\n",
    "            project_data[i]['evaluations']['train_set']['Ensemble']['auc'] = evaluation_trainset_ensemble['object']['result']['model']['average_area_under_roc_curve']\n",
    "            save_jar(jar_filename, project_data)\n",
    "        \n",
    "        ### Deepnet\n",
    "        if 'evaluation_id' not in project_data[i]['evaluations']['train_set']['Deepnet']:\n",
    "            evaluation_trainset_deepnet = api.create_evaluation(deepnet, dataset_train_train_split)\n",
    "            project_data[i]['evaluations']['train_set']['Deepnet']['evaluation_id'] = evaluation_trainset_deepnet['resource']\n",
    "            save_jar(jar_filename, project_data)\n",
    "            api.ok(evaluation_trainset_deepnet)\n",
    "            project_data[i]['evaluations']['train_set']['Deepnet']['auc'] = evaluation_trainset_deepnet['object']['result']['model']['average_area_under_roc_curve']\n",
    "        else:\n",
    "            evaluation_trainset_deepnet = api.get_evaluation(project_data[i]['evaluations']['train_set']['Deepnet']['evaluation_id'])\n",
    "            api.ok(evaluation_trainset_deepnet)\n",
    "            project_data[i]['evaluations']['train_set']['Deepnet']['auc'] = evaluation_trainset_deepnet['object']['result']['model']['average_area_under_roc_curve']\n",
    "            save_jar(jar_filename, project_data)\n",
    "        \n",
    "        print(f\" >>>> Evaluation {i} : OK\")\n",
    "    i += 1\n",
    "\n",
    "# On enregistre le dictionnaire project_data dans un fichier via la fonction dump\n",
    "save_jar(jar_filename, project_data)\n",
    "\n",
    "print(' >>>>> Terminé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC VAL SET Ensemble : [0.85126, 0.86592, 0.87131, 0.87473, 0.87661, 0.87785, 0.8784, 0.87832, 0.87888, 0.87958]\n",
      "AUC VAL SET Deepnet : [0.8505, 0.85523, 0.85222, 0.84855, 0.85047, 0.85046, 0.846, 0.85234, 0.84934, 0.85519]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC VAL SET Ensemble : [0.85126, 0.86592, 0.87131, 0.87473, 0.87661, 0.87785, 0.8784, 0.87832, 0.87888, 0.87958]\n",
      "AUC TRAIN SET Ensemble : [0.96792, 0.92717, 0.90926, 0.8981, 0.8954, 0.89017, 0.88585, 0.88236, 0.88072, 0.87958]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC VAL SET Deepnet : [0.8505, 0.85523, 0.85222, 0.84855, 0.85047, 0.85046, 0.846, 0.85234, 0.84934, 0.85519]\n",
      "AUC TRAIN SET Deepnet : [0.88429, 0.87537, 0.86295, 0.85242, 0.85554, 0.85443, 0.84697, 0.85196, 0.84921, 0.85519]\n"
     ]
    }
   ],
   "source": [
    "# On ouvre le fichier contenant les datas du projet, notamment les AUC\n",
    "project_data = read_jar(jar_filename)\n",
    "    \n",
    "# On initialise les listes graph_x et graph_y\n",
    "graph_x = []\n",
    "graph_y_valset_ensemble = []\n",
    "graph_y_valset_deepnet = []\n",
    "graph_y_trainset_ensemble = []\n",
    "graph_y_trainset_deepnet = []\n",
    "\n",
    "# On complète les listes depuis les datas du projet\n",
    "i = 0\n",
    "for project_data_auc in project_data.values():\n",
    "    if i != 0: # On saute la première boucle car elle contient les informations sur les sources et les dataset de base\n",
    "        graph_x.append(i)\n",
    "        graph_y_valset_ensemble.append(project_data_auc['evaluations']['val_set']['Ensemble']['auc'])\n",
    "        graph_y_valset_deepnet.append(project_data_auc['evaluations']['val_set']['Deepnet']['auc'])\n",
    "        graph_y_trainset_ensemble.append(project_data_auc['evaluations']['train_set']['Ensemble']['auc'])\n",
    "        graph_y_trainset_deepnet.append(project_data_auc['evaluations']['train_set']['Deepnet']['auc'])\n",
    "    i += 10\n",
    "\n",
    "\n",
    "# On génère le graphique Learning Curve (AUC / % Split)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(graph_x, graph_y_valset_ensemble, label='Ensemble')\n",
    "ax.plot(graph_x, graph_y_valset_deepnet, label='Deepnet')\n",
    "ax.axhline(y=0.87, color='r', label='Best AUC Kaggle')\n",
    "ax.set(xlim=(0, 120), ylim=(0.83, 0.885), xlabel='Taille du dataset en % du dataset_traint_train', ylabel='AUC',  title='Learning Curve (AUC / % Split)')\n",
    "plt.legend(bbox_to_anchor=(0.98, 0.98), loc='best', borderaxespad=0.)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "print(f\"AUC VAL SET Ensemble : {graph_y_valset_ensemble}\")\n",
    "print(f\"AUC VAL SET Deepnet : {graph_y_valset_deepnet}\")\n",
    "\n",
    "# On génère le graphique ENSEMBLE - Learning Curve (AUC / % Split)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(graph_x, graph_y_valset_ensemble, label='Training Error')\n",
    "ax.plot(graph_x, graph_y_trainset_ensemble, label='Dev Error')\n",
    "ax.axhline(y=0.87, color='r', label='Best AUC Kaggle')\n",
    "ax.set(xlim=(0, 120), ylim=(0.8, 1), xlabel='Taille du dataset en % du dataset_traint_train', ylabel='AUC',  title='ENSEMBLE - Learning Curve (AUC / % Split)')\n",
    "plt.legend(bbox_to_anchor=(0.98, 0.98), loc='best', borderaxespad=0.)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC VAL SET Ensemble : {graph_y_valset_ensemble}\")\n",
    "print(f\"AUC TRAIN SET Ensemble : {graph_y_trainset_ensemble}\")\n",
    "\n",
    "# On génère le graphique DEEPNET - Learning Curve (AUC / % Split)\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(graph_x, graph_y_valset_deepnet, label='Training Error')\n",
    "ax.plot(graph_x, graph_y_trainset_deepnet, label='Dev Error')\n",
    "ax.axhline(y=0.87, color='r', label='Best AUC Kaggle')\n",
    "ax.set(xlim=(0, 120), ylim=(0.8, 1), xlabel='Taille du dataset en % du dataset_traint_train', ylabel='AUC',  title='DEEPNET - Learning Curve (AUC / % Split)')\n",
    "plt.legend(bbox_to_anchor=(0.98, 0.98), loc='best', borderaxespad=0.)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC VAL SET Deepnet : {graph_y_valset_deepnet}\")\n",
    "print(f\"AUC TRAIN SET Deepnet : {graph_y_trainset_deepnet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
